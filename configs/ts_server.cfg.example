{
  log_filename: "/dev/stdout",

  /* if true, enable GPU usage */
//  cuda: true,
  /* cuda device index, use it if multiple GPUs */
//  device_index: 0,

  /* maximum number of threads, only matters when running on CPU */
  n_threads: 4,
  
  /* models to load. 'name' is the identifier used in the JSON
     request. 'filename' is the file containing the model description
   */
  models: [
//    { name: "gptj_6B_q4",  filename: "./models/gptj_6B_q4.bin" },
//    { name: "flan_t5_xxl_q4", filename: "./models/flan_t5_xxl_q4.bin" },
  ],

  local_port: 8080, /* port on which the server listen to */
  log_start: true, /* print "Started." when the server is ready */
  gui: true, /* start a simple GUI when exploring the root path
               (e.g. http://localhost:8080 here) (default = false) */
}
